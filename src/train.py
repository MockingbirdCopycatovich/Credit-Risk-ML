import numpy as np
import joblib

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score

from lightgbm import LGBMClassifier

from src.data_loader import load_application_train
from src.preprocessing import clean_data


def train():
    """
    Train LightGBM model
    using 5-fold Stratified Cross-Validation,
    compute OOF predictions and evaluate ROC-AUC.
    """

    # Load dataset
    df = load_application_train("data/raw")

    # Apply cleaning and feature engineering
    df = clean_data(df)

    # Separate features and target
    X = df.drop(columns=["TARGET"])
    y = df["TARGET"]

    # Identify numerical and categorical columns
    num_cols = X.select_dtypes(include=["int64", "float64"]).columns
    cat_cols = X.select_dtypes(include=["object", "string"]).columns

    # Preprocessing pipelines
    # Numerical pipeline:
    # - Fill missing values with median
    # - Scale features
    numeric_transformer = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler())
    ])

    # Categorical pipeline:
    # - Fill missing values with "Unknown"
    # - Apply One-Hot Encoding
    categorical_transformer = Pipeline([
        ("imputer", SimpleImputer(strategy="constant", fill_value="Unknown")),
        ("encoder", OneHotEncoder(handle_unknown="ignore"))
    ])

    # Combine numerical and categorical preprocessing
    preprocessor = ColumnTransformer([
        ("num", numeric_transformer, num_cols),
        ("cat", categorical_transformer, cat_cols)
    ])

    # Define LightGBM model
    # LightGBM handles non-linear relationships and works well for tabular data
    classifier = LGBMClassifier(
        n_estimators=1000,
        learning_rate=0.05,
        num_leaves=31,
        class_weight="balanced",
        random_state=42,
        n_jobs=-1
    )

    # Combine preprocessing and model into a single Pipeline
    model = Pipeline([
        ("preprocessing", preprocessor),
        ("classifier", classifier)
    ])

    # Stratified 5-fold cross-validation
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    # Initialize OOF predictions array
    # This will store predictions for each sample
    # generated by a model that did NOT see that sample during training
    oof_preds = np.zeros(len(X))

    scores = []

    # Cross-validation loop
    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):

        # Split data into training and validation sets
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        # Train model on training fold
        model.fit(X_train, y_train)

        # Predict probabilities for validation fold
        preds = model.predict_proba(X_val)[:, 1]

        # Store OOF predictions
        oof_preds[val_idx] = preds

        # Compute ROC-AUC for this fold
        fold_score = roc_auc_score(y_val, preds)
        scores.append(fold_score)

        print(f"Fold {fold} ROC-AUC: {fold_score:.4f}")

    # Compute overall OOF ROC-AUC
    final_score = roc_auc_score(y, oof_preds)

    print("-" * 40)
    print(f"Mean CV ROC-AUC: {np.mean(scores):.4f} Â± {np.std(scores):.4f}")
    print(f"OOF ROC-AUC: {final_score:.4f}")

    # Retrain model on full dataset before saving
    model.fit(X, y)

    # Save trained model for future inference / API usage
    joblib.dump(model, "models/lgbm_model.pkl")

    print("Model saved to models/lgbm_model.pkl")


if __name__ == "__main__":
    train()